<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>seaandsailor</title><link href="/" rel="alternate"></link><link href="/feeds/ift6266.atom.xml" rel="self"></link><id>/</id><updated>2014-02-01T18:00:00-05:00</updated><entry><title>Speech signal representations</title><link href="/initial_representation.html" rel="alternate"></link><updated>2014-02-01T18:00:00-05:00</updated><author><name>jfsantos</name></author><id>tag:,2014-02-01:initial_representation.html</id><summary type="html">&lt;p&gt;One of the objectives of our project is to learn a useful
representation from speech signals that can be used to synthesize new
(arbitrary) sentences. There are many different ways of representing
speech signals; those representations are usually tailored to specific
applications. In speech recognition, for example, we want to minimize
the variability from different speakers while keeping sufficient
information to discriminate different phonemes. In speech coding,
however, we usually want to keep information that is associated with
the speaker's identity as well as reduce the amount of data to be
stored/transmitted.&lt;/p&gt;
&lt;p&gt;Our dataset was initially distributed as frame MFCCs (input) and
one-hot encoded phonemes (labels). While this representation is
usually enough for speech recognition, I believe it is not enough for
learning a useful representation for synthesis (as briefly mentioned
by Laurent Dinh in his &lt;a class="reference external" href="http://deeprandommumbling.wordpress.com/2014/01/29/listening-to-a-vector"&gt;post&lt;/a&gt;). The reason is that MFCCs are a
destructive/lossy representation of a speech signal. First,
fundamental frequency information is completely lost, as well as
instantaneous phase. MFCCs more or less represent the energy in
different frequency channels that are considered important for human
speech (following the Mel scale &lt;a class="citation-reference" href="#stevens2005" id="id1"&gt;[Stevens2005]&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;In this post, I will present some alternative speech signal
representations that may be more suitable for speech synthesis. Even
though one of our objectives is to learn a representation, we need to
understand a little bit about what has been developed by the speech
processing community, as it can serve as an inspiration.&lt;/p&gt;
&lt;div class="section" id="acoustic-samples-time-domain"&gt;
&lt;h2&gt;Acoustic samples (time domain)&lt;/h2&gt;
&lt;p&gt;Using raw acoustic samples from overlapping frames is the simplest
approach. A discrete signal &lt;span class="math"&gt;\(x[n]\)&lt;/span&gt;
 is simply a sequence of (real
or integer) numbers corresponding to the signal samples (sampled
uniformly at an arbitrary sampling rate). The usual sampling rate for
speech recognition applications is 16 kHz, while the sampling rate
used for &amp;quot;telephone speech&amp;quot; coding is 8 kHz. This is essentially the
information we find in a PCM-encoded WAV file.&lt;/p&gt;
&lt;!-- add plots --&gt;
&lt;div class="figure align-center"&gt;
&lt;img alt="" src="images/timedomain.png" /&gt;
&lt;p class="caption"&gt;&lt;em&gt;Time-domain speech signal sampled at 16 kHz.&lt;/em&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div class="figure align-center"&gt;
&lt;img alt="" src="images/timedomain_zoom.png" /&gt;
&lt;p class="caption"&gt;&lt;em&gt;Zoom of a 200-sample segment of the above signal.&lt;/em&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="section" id="short-time-fourier-transform"&gt;
&lt;h2&gt;Short-time Fourier Transform&lt;/h2&gt;
&lt;p&gt;Another possible representation is to use Short-time Fourier Transform
(STFT) coefficients from overlapping frames. This is essentially the
same as using raw acoustic samples in the sense that there is no
information loss, but the representation in the frequency domain is
usually clearer for humans because we can associate the content in
different frequency bands with different phonemes. The STFT of a
discrete signal &lt;span class="math"&gt;\(x[n]\)&lt;/span&gt;
 is given by:&lt;/p&gt;
&lt;div class="math"&gt;
\begin{equation*}
STFT{x[n]}(m,\omega) = X(m,\omega) = \sum_{n=-\infty}^{\infty} x[n] w[n-m] e^{-j \omega n}
\end{equation*}
&lt;/div&gt;
&lt;p&gt;where &lt;span class="math"&gt;\(n,\omega\)&lt;/span&gt;
 are the time and frequency indexes, and
&lt;span class="math"&gt;\(w[n]\)&lt;/span&gt;
 is the windowing function. A spectrogram is the
magnitude-squared version of this equation (i.e., without phase
information).&lt;/p&gt;
&lt;p&gt;Spectrograms can be done using windows with different lengths. This is
related to the &lt;a class="reference external" href="http://en.wikipedia.org/wiki/Short-time_Fourier_transform#Resolution_issues"&gt;Gabor (or Heisenberg-Gabor)&lt;/a&gt; limit: we cannot
simultaneously localize a signal in both time and frequency domains
with a high degree of certainty. Therefore, we usually have to use
different window lengths depending on what we want to analyze: wide
windows have better frequency resolution and bad time resolution,
while the opposite happens for short windows. A possible compromise is
to choose a single window length that has sufficient resolution for
the target application.&lt;/p&gt;
&lt;div class="figure align-center"&gt;
&lt;img alt="" src="images/specgram.png" /&gt;
&lt;p class="caption"&gt;&lt;em&gt;Spectrogram (using a 20 ms rectangular window) of the speech signal above.&lt;/em&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="section" id="linear-predictive-coding"&gt;
&lt;h2&gt;Linear Predictive Coding&lt;/h2&gt;
&lt;p&gt;Linear Predictive Coding &lt;a class="citation-reference" href="#o1988linear" id="id2"&gt;[o1988linear]&lt;/a&gt; coefficients + residual
(basically excitation information). LPC is based on the source-filter
model of speech production, which assumes a speech signal is produced
by filtering a series of pulses (and eventually noise bursts). The LPC
coefficients are related to the position of the articulators in the
mouth (e.g., tongue, palate, lips), while the pitch/noise information
is related to how the vocal tract is excited. This is usually
represented as an auto-regressive (AR) model with order &lt;span class="math"&gt;\(p\)&lt;/span&gt;
:&lt;/p&gt;
&lt;div class="math"&gt;
\begin{equation*}
x[n] = \sum_{k=1}^{p} a_k x[n-k]
\end{equation*}
&lt;/div&gt;
&lt;p&gt;where &lt;span class="math"&gt;\(a[k]\)&lt;/span&gt;
 are the model's coefficients. LPCs are computed for each speech frame based on a least-squares method:&lt;/p&gt;
&lt;div class="math"&gt;
\begin{equation*}
\arg\min_{a_k} \sum_{-\infty}^\infty [x[n] - \sum_{k=1}^p a_k x[n-k]]^2
\end{equation*}
&lt;/div&gt;
&lt;p&gt;Because of its error criteria, LPC also has problems to represent the
phase of acoustic signals (by squaring the error, we are modeling the
spectral magnitude of the signal, and not the phase). For this reason,
LPC speech may sound artificial when resynthesized. More robust
methods are used nowadays, such as the code-excited linear prediction
(CELP) &lt;a class="citation-reference" href="#valin2006speex" id="id3"&gt;[valin2006speex]&lt;/a&gt;. These methods, for example, use
psychoacoustics-inspired techniques to shape the coding noise to
frequency regions where the human auditory system is more tolerant. In
CELP, the residual is not transmitted directly, but represented as
entries in two codebooks.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="wavelets"&gt;
&lt;h2&gt;Wavelets&lt;/h2&gt;
&lt;p&gt;Them main purpose of a wavelet transform is to decompose arbitrary
signals into localized contributions that can be labelled by a scale
(or resolution) parameter &lt;a class="citation-reference" href="#mallat1989theory" id="id4"&gt;[mallat1989theory]&lt;/a&gt;. The representation
achieved through the wavelet transform can be seen as hierarchical: at
a coarse resolution, we have an idea of “context”, while with highest
resolution we can see more details. This is achieved by decomposing
the original signal using a set of functions well-localized both in
time and frequency (the so-called wavelets).&lt;/p&gt;
&lt;p&gt;Discrete wavelet transforms are implemented as a cascade of digital
filters with transfer functions derived from a discrete &amp;quot;mother
wavelet&amp;quot;. The figure below shows an example. Check also the &lt;a class="reference internal" href="#notebook"&gt;notebook&lt;/a&gt;
for an example of wavelet decomposition of the audio signal shown
above.&lt;/p&gt;
&lt;div class="figure align-center"&gt;
&lt;img alt="" src="images/Wavelets_-_Filter_Bank.png" /&gt;
&lt;p class="caption"&gt;&lt;em&gt;Filter bank used by a discrete wavelet transform with 3 levels of decomposition (image from the&lt;/em&gt; &lt;a class="reference external" href="http://en.wikipedia.org/wiki/File:Wavelets_-_Filter_Bank.png"&gt;WikiMedia Commons&lt;/a&gt; &lt;em&gt;)&lt;/em&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="section" id="sparse-coding-and-dictionary-based-methods"&gt;
&lt;h2&gt;Sparse coding and dictionary-based methods&lt;/h2&gt;
&lt;p&gt;Sparse signal approximations are the basis for a variety of signal
processing techniques. Such approximations are usually employed with
the objective of having a signal representation that is more
meaningful, malleable, and robust to noise than the ones obtained by
standard transform methods &lt;a class="citation-reference" href="#sturm" id="id5"&gt;[Sturm]&lt;/a&gt;. The so-called
dictionary-based methods (DBM) decompose a signal into a linear
combination of waveforms through an approximation technique such as
Matching Pursuit (MP) &lt;a class="citation-reference" href="#mallat1993" id="id6"&gt;[Mallat1993]&lt;/a&gt; or Orthogonal Matching Pursuit
(OMP) &lt;a class="citation-reference" href="#pati1993" id="id7"&gt;[Pati1993]&lt;/a&gt;. The collection of waveforms that can be
selected for the linear combination is called a dictionary. This
dictionary is usually overcomplete, either because it is formed by
merging complete dictionaries or because the functions are chosen
arbitrarily.&lt;/p&gt;
&lt;p&gt;I will talk more about sparse coding and dictionary-based methods
later, since sparse coding is one of the methods we'll see in the
course.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="ipython-notebook"&gt;
&lt;span id="notebook"&gt;&lt;/span&gt;&lt;h2&gt;IPython notebook&lt;/h2&gt;
&lt;p&gt;An IPython notebook with examples for all the representations
described here (except sparse coding) is available on my &lt;a class="reference external" href="https://github.com/jfsantos/ift6266h14"&gt;GitHub
repo&lt;/a&gt;. You will need to install the packages &lt;a class="reference external" href="http://www.pybytes.com/pywavelets/"&gt;PyWavelets&lt;/a&gt; and
&lt;a class="reference external" href="https://github.com/cournape/talkbox"&gt;scikits.talkbox&lt;/a&gt; (both are available at PyPI) to be able to run it. If you just want to take a look without interacting with the code, you can access it &lt;a class="reference external" href="http://nbviewer.ipython.org/github/jfsantos/ift6266h14/blob/master/notebooks/Speech%20representation%20examples.ipynb"&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="references"&gt;
&lt;h2&gt;References&lt;/h2&gt;
&lt;table class="docutils citation" frame="void" id="stevens2005" rules="none"&gt;
&lt;colgroup&gt;&lt;col class="label" /&gt;&lt;col /&gt;&lt;/colgroup&gt;
&lt;tbody valign="top"&gt;
&lt;tr&gt;&lt;td class="label"&gt;&lt;a class="fn-backref" href="#id1"&gt;[Stevens2005]&lt;/a&gt;&lt;/td&gt;&lt;td&gt;&lt;ol class="first last upperalpha simple" start="19"&gt;
&lt;li&gt;&lt;ol class="first upperalpha" start="19"&gt;
&lt;li&gt;Stevens, J. Volkmann, and E. B. Newman, “A Scale for the Measurement of the Psychological Magnitude Pitch,” The Journal of the Acoustical Society of America, vol. 8, no. 3, pp. 185–190, Jun. 2005.&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/td&gt;&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;table class="docutils citation" frame="void" id="o1988linear" rules="none"&gt;
&lt;colgroup&gt;&lt;col class="label" /&gt;&lt;col /&gt;&lt;/colgroup&gt;
&lt;tbody valign="top"&gt;
&lt;tr&gt;&lt;td class="label"&gt;&lt;a class="fn-backref" href="#id2"&gt;[o1988linear]&lt;/a&gt;&lt;/td&gt;&lt;td&gt;&lt;ol class="first last upperalpha simple" start="4"&gt;
&lt;li&gt;O’Shaughnessy, “Linear predictive coding,” IEEE Potentials, vol. 7, no. 1, pp. 29–32, Feb. 1988.&lt;/li&gt;
&lt;/ol&gt;
&lt;/td&gt;&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;table class="docutils citation" frame="void" id="valin2006speex" rules="none"&gt;
&lt;colgroup&gt;&lt;col class="label" /&gt;&lt;col /&gt;&lt;/colgroup&gt;
&lt;tbody valign="top"&gt;
&lt;tr&gt;&lt;td class="label"&gt;&lt;a class="fn-backref" href="#id3"&gt;[valin2006speex]&lt;/a&gt;&lt;/td&gt;&lt;td&gt;J.-M. Valin, “Speex: a free codec for free speech,” in Australian National Linux Conference, Dunedin, New Zealand, 2006.&lt;/td&gt;&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;table class="docutils citation" frame="void" id="mallat1989theory" rules="none"&gt;
&lt;colgroup&gt;&lt;col class="label" /&gt;&lt;col /&gt;&lt;/colgroup&gt;
&lt;tbody valign="top"&gt;
&lt;tr&gt;&lt;td class="label"&gt;&lt;a class="fn-backref" href="#id4"&gt;[mallat1989theory]&lt;/a&gt;&lt;/td&gt;&lt;td&gt;&lt;ol class="first last upperalpha simple" start="19"&gt;
&lt;li&gt;&lt;ol class="first upperalpha" start="7"&gt;
&lt;li&gt;Mallat, “A theory for multiresolution signal decomposition: the wavelet representation,” Pattern Analysis and Machine Intelligence, IEEE Transactions on, vol. 11, no. 7, pp. 674–693, 1989.&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/td&gt;&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;table class="docutils citation" frame="void" id="sturm" rules="none"&gt;
&lt;colgroup&gt;&lt;col class="label" /&gt;&lt;col /&gt;&lt;/colgroup&gt;
&lt;tbody valign="top"&gt;
&lt;tr&gt;&lt;td class="label"&gt;&lt;a class="fn-backref" href="#id5"&gt;[Sturm]&lt;/a&gt;&lt;/td&gt;&lt;td&gt;&lt;ol class="first last upperalpha simple" start="2"&gt;
&lt;li&gt;&lt;ol class="first upperalpha" start="12"&gt;
&lt;li&gt;Sturm, C. Roads, A. McLeran, and J. J. Shynk, “Analysis, Visualization, and Transformation of Audio Signals Using Dictionary-based Methods†,” Journal of New Music Research, vol. 38, no. 4, pp. 325–341, 2009.&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/td&gt;&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;table class="docutils citation" frame="void" id="mallat1993" rules="none"&gt;
&lt;colgroup&gt;&lt;col class="label" /&gt;&lt;col /&gt;&lt;/colgroup&gt;
&lt;tbody valign="top"&gt;
&lt;tr&gt;&lt;td class="label"&gt;&lt;a class="fn-backref" href="#id6"&gt;[Mallat1993]&lt;/a&gt;&lt;/td&gt;&lt;td&gt;&lt;ol class="first last upperalpha simple" start="19"&gt;
&lt;li&gt;&lt;ol class="first upperalpha" start="7"&gt;
&lt;li&gt;Mallat and Z. Zhang, “Matching pursuits with time-frequency dictionaries,” IEEE Transactions on Signal Processing, vol. 41, no. 12, pp. 3397–3415, Dec. 1993.&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/td&gt;&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;table class="docutils citation" frame="void" id="pati1993" rules="none"&gt;
&lt;colgroup&gt;&lt;col class="label" /&gt;&lt;col /&gt;&lt;/colgroup&gt;
&lt;tbody valign="top"&gt;
&lt;tr&gt;&lt;td class="label"&gt;&lt;a class="fn-backref" href="#id7"&gt;[Pati1993]&lt;/a&gt;&lt;/td&gt;&lt;td&gt;&lt;ol class="first last upperalpha simple" start="25"&gt;
&lt;li&gt;&lt;ol class="first upperalpha" start="3"&gt;
&lt;li&gt;Pati, R. Rezaiifar, and P. S. Krishnaprasad, “Orthogonal matching pursuit: Recursive function approximation with applications to wavelet decomposition,” in Signals, Systems and Computers, 1993. 1993 Conference Record of The Twenty-Seventh Asilomar Conference on, 1993, pp. 40–44.&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/td&gt;&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
</summary><category term="ift6266"></category></entry><entry><title>Personal research journal: deep learning for speech synthesis</title><link href="/intro_ift6266.html" rel="alternate"></link><updated>2014-01-25T15:00:00-05:00</updated><author><name>jfsantos</name></author><id>tag:,2014-01-25:intro_ift6266.html</id><summary type="html">&lt;p&gt;This is the introduction to a series of reports on my experiments on
deep learning methods for speech synthesis. These experiments are part
of my coursework for Dr. Yoshua Bengio's &lt;a class="reference external" href="http://ift6266h14.wordpress.com"&gt;Representation Learning&lt;/a&gt;
course at Université de Montréal. All the related code is going to be
posted at a &lt;a class="reference external" href="https://github.com/jfsantos/ift6266h14"&gt;GitHub repository&lt;/a&gt; as well.&lt;/p&gt;
&lt;p&gt;Please visit the &lt;a class="reference external" href="/tag/ift6266.html"&gt;ift6266 tag page&lt;/a&gt; for a list of all the posts
related to this project.&lt;/p&gt;
</summary><category term="ift6266"></category></entry></feed>