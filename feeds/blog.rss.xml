<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>seaandsailor</title><link>/</link><description></description><atom:link href="/feeds/blog.rss.xml" rel="self"></atom:link><lastBuildDate>Tue, 25 Feb 2014 17:00:00 -0500</lastBuildDate><item><title>Dictionary Learning and Sparse Coding for Speech Signals</title><link>/dictlearning.html</link><description>&lt;p&gt;Sparse signal approximations are the basis for a variety of signal
processing techniques. Such approximations are usually employed with
the objective of having a signal representation that is more
meaningful, malleable, and robust to noise than the ones obtained by
standard transform methods &lt;a class="citation-reference" href="#sturm2009" id="id1"&gt;[Sturm2009]&lt;/a&gt;. The so-called dictionary
based methods (DBM) decompose a signal into a linear combination of
waveforms through an approximation technique such as Matching Pursuit
(MP) &lt;a class="citation-reference" href="#mallat1993" id="id2"&gt;[Mallat1993]&lt;/a&gt;, Orthogonal Matching Pursuit (OMP) &lt;a class="citation-reference" href="#pati1993" id="id3"&gt;[Pati1993]&lt;/a&gt;, or
basis pursuit &lt;a class="citation-reference" href="#chen2001" id="id4"&gt;[Chen2001]&lt;/a&gt;. The collection of waveforms that can be
selected for the linear combination is called a dictionary. This
dictionary is usually overcomplete, either because it is formed by
merging complete dictionaries or because the waveforms are chosen
arbitrarily (and we have more waveforms than the length of the signal
we want to represent).&lt;/p&gt;
&lt;div class="section" id="sparse-approximation-problem-formulations"&gt;
&lt;h2&gt;Sparse approximation problem formulations&lt;/h2&gt;
&lt;p&gt;The sparse coding problem is usually formulated either as a
sparsity-constrained problem or as an error-constrained problem. The
formulations are as follows:&lt;/p&gt;
&lt;p&gt;Sparsity-constrained:
&lt;span class="math"&gt;\(\underline{\hat{\gamma}} = \underset{\underline{\gamma}}{arg\,min}\|\underline{x} - D \underline{\gamma}\|_2^2 \quad\text{s.t.}\quad   \|\underline{\gamma}\|_0 \leq K\)&lt;/span&gt;
&lt;/p&gt;
&lt;p&gt;Error-constrained:
&lt;span class="math"&gt;\(\underline{\hat{\gamma}} = \underset{\underline{\gamma}}{arg\,min}\|\underline{\gamma}\|_0 \quad\text{s.t.}\quad \|\underline{x} - D \underline{\gamma}\|_2^2 \leq \epsilon\)&lt;/span&gt;
&lt;/p&gt;
&lt;p&gt;In the first one, the idea is that we want to represent the signal by
a linear combination of up to K known waveforms. In the second
formulation, we want the squared error of the representation to be
below a certain threshold. Both formulations are useful, depending on
the problem you are trying to solve: the first one will lead to more
compact representations, while with the second one you can avoid
higher representation errors.&lt;/p&gt;
&lt;p&gt;The second formulation is also useful for applications which need
denoising: consider you have a corrupted version of your signal, and
also that you know (more or less) the signal-to-noise ratio (SNR). If
the noise is very different from the signal you are interested in and
your dictionary is optimized to represent these signals only, it may
be the case that noise is not well represented by the waveforms in
your dictionary. So, you could use the second formulation, setting
&lt;span class="math"&gt;\(\epsilon\)&lt;/span&gt;
 as the estimated noise level, and expect that a good
part of the noise component is not going to be represented in the
sparse approximation.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="dictionary-learning"&gt;
&lt;h2&gt;Dictionary learning&lt;/h2&gt;
&lt;p&gt;Reconstructing a speech signal based on a learned set of segments is not
a new thing. It is done in a well-known technique called vector
quantization (VQ). In VQ, the signal is reconstructed by using only a
single atom (or &lt;em&gt;codeword&lt;/em&gt;, on the VQ literature jargon) per signal
frame. The dictionary (or &lt;em&gt;codebook&lt;/em&gt;) is usually designed by a
nearest-neighbor method, which aims to find the codebook that can
reconstruct a signal by using the codewords that have the smaller
distances to the original signal frames while minimizing the residual.
K-means is a codebook learning algorithm for VQ that solves this problem
by dividing the training samples into &lt;span class="math"&gt;\(K\)&lt;/span&gt;
 clusters of the nearest
neighbors of each of the &lt;span class="math"&gt;\(K\)&lt;/span&gt;
 items in the initial codebook. The
codebook is then updated by finding the centroid for each of the
&lt;span class="math"&gt;\(K\)&lt;/span&gt;
 clusters. These steps are ran iteratively until the algorithm
converges to a local minimum solution.&lt;/p&gt;
&lt;p&gt;For sparse coding, we want to use multiple atoms to reconstruct the
signal. In the snippet below, we generate a dictionary with 1024
waveforms by using the dictionary learning functions available in
&lt;a class="reference external" href="http://scikit-learn.org"&gt;scikit-learn&lt;/a&gt;, which is based on a paper by [Mairal et al]_. The
training data consists of two minutes of audio from the TIMIT
database; sentences were randomly chosen and then split into frames of
256 samples each.&lt;/p&gt;
&lt;pre class="code python literal-block"&gt;
&lt;span class="c"&gt;# Build the dictionary&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;sklearn.decomposition&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;MiniBatchDictionaryLearning&lt;/span&gt;
&lt;span class="n"&gt;dico&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;MiniBatchDictionaryLearning&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n_components&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1024&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;alpha&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;n_iter&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;100&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;D&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;dico&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;training_data&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;components_&lt;/span&gt;
&lt;/pre&gt;
&lt;img alt="" class="align-center" src="images/dictlearning_5_1.png" style="width: 720px;" /&gt;
&lt;p&gt;If we take a look into some of the learned waveforms in the figure
above, we'll see that we have both low-frequency, quasiperiodic
signals (which are probably matching vowels) and signals with more
high-frequency components that look a bit noisy (probably representing
stops/fricatives).&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="reconstructing-speech-segments-using-sparse-coding-with-the-learned-dictionary"&gt;
&lt;h2&gt;Reconstructing speech segments using sparse coding with the learned dictionary&lt;/h2&gt;
&lt;p&gt;Now that we have a dictionary which (supposedly) is good for
representing speech signals, let's use Orthogonal Matching Pursuit
(OMP) to reconstruct a speech segment based on a linear combination of
dictionary entries. Let's get 10 seconds of audio from TIMIT (from a
segment of the set that was not in the training set) and reconstruct
it using a sparse approximation. We use the sparsity-based constraint
form, as we are more interested in representing speech in a sparse
way:&lt;/p&gt;
&lt;pre class="code python literal-block"&gt;
&lt;span class="c"&gt;# Get sample speech segment to reconstruct&lt;/span&gt;
&lt;span class="n"&gt;test_data&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;fs&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="mi"&gt;200&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="n"&gt;fs&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="mi"&gt;210&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;reshape&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;fs&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="mi"&gt;256&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;256&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c"&gt;# Reconstruct it frame-by-frame using a linear combination of 20&lt;/span&gt;
&lt;span class="c"&gt;# atoms per frame (sparsity-constrained OMP)&lt;/span&gt;
&lt;span class="n"&gt;result&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;numpy&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ndarray&lt;/span&gt;&lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="n"&gt;test_data&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;&lt;span class="mi"&gt;512&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;

&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;sklearn.decomposition&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;SparseCoder&lt;/span&gt;

&lt;span class="n"&gt;coder&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;SparseCoder&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;dictionary&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;D&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;transform_n_nonzero_coefs&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;20&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                    &lt;span class="n"&gt;transform_alpha&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;None&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;transform_algorithm&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;omp&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;result&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;coder&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;transform&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;test_data&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;n&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;result&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]):&lt;/span&gt;
    &lt;span class="n"&gt;out&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;n&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="mi"&gt;256&lt;/span&gt;&lt;span class="p"&gt;:(&lt;/span&gt;&lt;span class="n"&gt;n&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="mi"&gt;256&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;D&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;T&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;result&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;n&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;&lt;span class="n"&gt;axis&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;
&lt;p&gt;Here are the results: you can listen above the original file and the reconstructed one.&lt;/p&gt;
&lt;p&gt; Original: &lt;br&gt;
&lt;audio controls="controls" &gt;
      &lt;source src="files/orig.ogg" type="audio/wav" /&gt;
      Your browser does not support the audio element.
&lt;/audio&gt; &lt;/p&gt;
&lt;p&gt; Reconstructed with 20 atoms/frame:&lt;br&gt;
&lt;audio controls="controls" &gt;
      &lt;source src="files/reconst.ogg" type="audio/wav" /&gt;
      Your browser does not support the audio element.
&lt;/audio&gt;&lt;/p&gt;&lt;p&gt;These figures show the original signal, the reconstructed one, and the squared error:&lt;/p&gt;
&lt;img alt="" class="align-center" src="images/dictlearning_10_1.png" /&gt;
&lt;p&gt;While the reconstruction error is low for most of the time considering
we are using only 20 non-zero values per frame to represent the
signal, as opposed to using 256 samples, we can clearly hear the
reconstruction-related artifacts. However, that may be OK if all we
want with the learned dictionary is to have a sparser representation
for speech that will be used later in our synthesizer.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="relationship-with-our-project-and-next-steps"&gt;
&lt;h2&gt;Relationship with our project and next steps&lt;/h2&gt;
&lt;p&gt;I started working on some experiments comparing the performance of a
sample predictor to two other predictors: one based on LPC
coefficients and the other on a sparse representation of speech. As we
discussed in class, speech has some parameters that change quickly
(source/excitation signal), while others change slowly
(articulation-related). In the first experiments prof. Bengio
suggested, we were working on an MLP-based generative model for
samples without any consideration for phones. His second suggestion
was to design a generative model for the next sample conditioned on
the previous, current, and next phone.&lt;/p&gt;
&lt;p&gt;I started developing generative models based on MLPs for the three
representations above, using one-hot encoded phones and the relative
position in time of the current phone as inputs. For the model based
on LPCs, I am planning to have a separate generative model for the
excitation signal, which is going to work pretty much like the
next-sample predictor we worked on previously; this model could also
be based on the previous, current, and next phone, previous samples,
and things such as pitch/speaker gender. Unfortunately, due to a &lt;a class="reference external" href="https://groups.google.com/forum/#!topic/pylearn-users/EZ3H8xP7gN8"&gt;bug&lt;/a&gt;
in pylearn2 I was not able to get them working yet. &lt;a class="reference external" href="http://vdumoulin.github.io/"&gt;Vincent&lt;/a&gt; said
there's already a &lt;a class="reference external" href="https://github.com/lisa-lab/pylearn2/pull/512"&gt;pull request&lt;/a&gt; which solves this
issue and it seems it will get fixed anytime soon.&lt;/p&gt;
&lt;p&gt;Last note: you can view the IPython notebook containing all the code used to generate the dictionary and the plots &lt;a class="reference external" href="http://nbviewer.ipython.org/urls/seaandsailor.com/files/dictlearning.ipynb"&gt;here&lt;/a&gt;, or &lt;a class="reference external" href="files/dictlearning.ipynb"&gt;download&lt;/a&gt; and run it interactively in your computer.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="references"&gt;
&lt;h2&gt;References&lt;/h2&gt;
&lt;table class="docutils citation" frame="void" id="sturm2009" rules="none"&gt;
&lt;colgroup&gt;&lt;col class="label" /&gt;&lt;col /&gt;&lt;/colgroup&gt;
&lt;tbody valign="top"&gt;
&lt;tr&gt;&lt;td class="label"&gt;&lt;a class="fn-backref" href="#id1"&gt;[Sturm2009]&lt;/a&gt;&lt;/td&gt;&lt;td&gt;&lt;ol class="first last upperalpha simple" start="2"&gt;
&lt;li&gt;&lt;ol class="first upperalpha" start="12"&gt;
&lt;li&gt;Sturm, C. Roads, A. McLeran, and J. J. Shynk, “Analysis, Visualization, and Transformation of Audio Signals Using Dictionary-based Methods†,” Journal of New Music Research, vol. 38, no. 4, pp. 325–341, 2009.&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/td&gt;&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;table class="docutils citation" frame="void" id="mallat1993" rules="none"&gt;
&lt;colgroup&gt;&lt;col class="label" /&gt;&lt;col /&gt;&lt;/colgroup&gt;
&lt;tbody valign="top"&gt;
&lt;tr&gt;&lt;td class="label"&gt;&lt;a class="fn-backref" href="#id2"&gt;[Mallat1993]&lt;/a&gt;&lt;/td&gt;&lt;td&gt;&lt;ol class="first last upperalpha simple" start="19"&gt;
&lt;li&gt;&lt;ol class="first upperalpha" start="7"&gt;
&lt;li&gt;Mallat and Z. Zhang, “Matching pursuits with time-frequency dictionaries,” IEEE Transactions on Signal Processing, vol. 41, no. 12, pp. 3397–3415, Dec. 1993.&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/td&gt;&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;table class="docutils citation" frame="void" id="pati1993" rules="none"&gt;
&lt;colgroup&gt;&lt;col class="label" /&gt;&lt;col /&gt;&lt;/colgroup&gt;
&lt;tbody valign="top"&gt;
&lt;tr&gt;&lt;td class="label"&gt;&lt;a class="fn-backref" href="#id3"&gt;[Pati1993]&lt;/a&gt;&lt;/td&gt;&lt;td&gt;&lt;ol class="first last upperalpha simple" start="25"&gt;
&lt;li&gt;&lt;ol class="first upperalpha" start="3"&gt;
&lt;li&gt;Pati, R. Rezaiifar, and P. S. Krishnaprasad, “Orthogonal matching pursuit: Recursive function approximation with applications to wavelet decomposition,” in Signals, Systems and Computers, 1993. 1993 Conference Record of The Twenty-Seventh Asilomar Conference on, 1993, pp. 40–44.&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/td&gt;&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;table class="docutils citation" frame="void" id="chen2001" rules="none"&gt;
&lt;colgroup&gt;&lt;col class="label" /&gt;&lt;col /&gt;&lt;/colgroup&gt;
&lt;tbody valign="top"&gt;
&lt;tr&gt;&lt;td class="label"&gt;&lt;a class="fn-backref" href="#id4"&gt;[Chen2001]&lt;/a&gt;&lt;/td&gt;&lt;td&gt;&lt;ol class="first last upperalpha simple" start="19"&gt;
&lt;li&gt;&lt;ol class="first upperalpha" start="19"&gt;
&lt;li&gt;Chen, D. L. Donoho, and M. A. Saunders, “Atomic decomposition by basis pursuit,” SIAM journal on scientific computing, vol. 20, no. 1, pp. 33–61, 1998.&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/td&gt;&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;table class="docutils citation" frame="void" id="mairal2009" rules="none"&gt;
&lt;colgroup&gt;&lt;col class="label" /&gt;&lt;col /&gt;&lt;/colgroup&gt;
&lt;tbody valign="top"&gt;
&lt;tr&gt;&lt;td class="label"&gt;[Mairal2009]&lt;/td&gt;&lt;td&gt;&lt;ol class="first last upperalpha simple" start="10"&gt;
&lt;li&gt;Mairal, F. Bach, J. Ponce, and G. Sapiro, “Online dictionary learning for sparse coding,” in Proceedings of the 26th Annual International Conference on Machine Learning, 2009, pp. 689–696.&lt;/li&gt;
&lt;/ol&gt;
&lt;/td&gt;&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">jfsantos</dc:creator><pubDate>Tue, 25 Feb 2014 17:00:00 -0500</pubDate><guid>tag:,2014-02-25:dictlearning.html</guid><category>ift6266</category></item><item><title>Speech signal representations</title><link>/initial_representation.html</link><description>&lt;p&gt;One of the objectives of our project is to learn a useful
representation from speech signals that can be used to synthesize new
(arbitrary) sentences. There are many different ways of representing
speech signals; those representations are usually tailored to specific
applications. In speech recognition, for example, we want to minimize
the variability from different speakers while keeping sufficient
information to discriminate different phonemes. In speech coding,
however, we usually want to keep information that is associated with
the speaker's identity as well as reduce the amount of data to be
stored/transmitted.&lt;/p&gt;
&lt;p&gt;Our dataset was initially distributed as frame MFCCs (input) and
one-hot encoded phonemes (labels). While this representation is
usually enough for speech recognition, I believe it is not enough for
learning a useful representation for synthesis (as briefly mentioned
by Laurent Dinh in his &lt;a class="reference external" href="http://deeprandommumbling.wordpress.com/2014/01/29/listening-to-a-vector"&gt;post&lt;/a&gt;). The reason is that MFCCs are a
destructive/lossy representation of a speech signal. First,
fundamental frequency information is completely lost, as well as
instantaneous phase. MFCCs more or less represent the energy in
different frequency channels that are considered important for human
speech (following the Mel scale &lt;a class="citation-reference" href="#stevens2005" id="id1"&gt;[Stevens2005]&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;In this post, I will present some alternative speech signal
representations that may be more suitable for speech synthesis. Even
though one of our objectives is to learn a representation, we need to
understand a little bit about what has been developed by the speech
processing community, as it can serve as an inspiration.&lt;/p&gt;
&lt;div class="section" id="acoustic-samples-time-domain"&gt;
&lt;h2&gt;Acoustic samples (time domain)&lt;/h2&gt;
&lt;p&gt;Using raw acoustic samples from overlapping frames is the simplest
approach. A discrete signal &lt;span class="math"&gt;\(x[n]\)&lt;/span&gt;
 is simply a sequence of (real
or integer) numbers corresponding to the signal samples (sampled
uniformly at an arbitrary sampling rate). The usual sampling rate for
speech recognition applications is 16 kHz, while the sampling rate
used for &amp;quot;telephone speech&amp;quot; coding is 8 kHz. This is essentially the
information we find in a PCM-encoded WAV file.&lt;/p&gt;
&lt;!-- add plots --&gt;
&lt;div class="figure align-center"&gt;
&lt;img alt="" src="images/timedomain.png" /&gt;
&lt;p class="caption"&gt;&lt;em&gt;Time-domain speech signal sampled at 16 kHz.&lt;/em&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div class="figure align-center"&gt;
&lt;img alt="" src="images/timedomain_zoom.png" /&gt;
&lt;p class="caption"&gt;&lt;em&gt;Zoom of a 200-sample segment of the above signal.&lt;/em&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="section" id="short-time-fourier-transform"&gt;
&lt;h2&gt;Short-time Fourier Transform&lt;/h2&gt;
&lt;p&gt;Another possible representation is to use Short-time Fourier Transform
(STFT) coefficients from overlapping frames. This is essentially the
same as using raw acoustic samples in the sense that there is no
information loss, but the representation in the frequency domain is
usually clearer for humans because we can associate the content in
different frequency bands with different phonemes. The STFT of a
discrete signal &lt;span class="math"&gt;\(x[n]\)&lt;/span&gt;
 is given by:&lt;/p&gt;
&lt;div class="math"&gt;
\begin{equation*}
STFT{x[n]}(m,\omega) = X(m,\omega) = \sum_{n=-\infty}^{\infty} x[n] w[n-m] e^{-j \omega n}
\end{equation*}
&lt;/div&gt;
&lt;p&gt;where &lt;span class="math"&gt;\(n,\omega\)&lt;/span&gt;
 are the time and frequency indexes, and
&lt;span class="math"&gt;\(w[n]\)&lt;/span&gt;
 is the windowing function. A spectrogram is the
magnitude-squared version of this equation (i.e., without phase
information).&lt;/p&gt;
&lt;p&gt;Spectrograms can be done using windows with different lengths. This is
related to the &lt;a class="reference external" href="http://en.wikipedia.org/wiki/Short-time_Fourier_transform#Resolution_issues"&gt;Gabor (or Heisenberg-Gabor)&lt;/a&gt; limit: we cannot
simultaneously localize a signal in both time and frequency domains
with a high degree of certainty. Therefore, we usually have to use
different window lengths depending on what we want to analyze: wide
windows have better frequency resolution and bad time resolution,
while the opposite happens for short windows. A possible compromise is
to choose a single window length that has sufficient resolution for
the target application.&lt;/p&gt;
&lt;div class="figure align-center"&gt;
&lt;img alt="" src="images/specgram.png" /&gt;
&lt;p class="caption"&gt;&lt;em&gt;Spectrogram (using a 20 ms rectangular window) of the speech signal above.&lt;/em&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="section" id="linear-predictive-coding"&gt;
&lt;h2&gt;Linear Predictive Coding&lt;/h2&gt;
&lt;p&gt;Linear Predictive Coding &lt;a class="citation-reference" href="#o1988linear" id="id2"&gt;[o1988linear]&lt;/a&gt; coefficients + residual
(basically excitation information). LPC is based on the source-filter
model of speech production, which assumes a speech signal is produced
by filtering a series of pulses (and eventually noise bursts). The LPC
coefficients are related to the position of the articulators in the
mouth (e.g., tongue, palate, lips), while the pitch/noise information
is related to how the vocal tract is excited. This is usually
represented as an auto-regressive (AR) model with order &lt;span class="math"&gt;\(p\)&lt;/span&gt;
:&lt;/p&gt;
&lt;div class="math"&gt;
\begin{equation*}
x[n] = \sum_{k=1}^{p} a_k x[n-k]
\end{equation*}
&lt;/div&gt;
&lt;p&gt;where &lt;span class="math"&gt;\(a[k]\)&lt;/span&gt;
 are the model's coefficients. LPCs are computed for each speech frame based on a least-squares method:&lt;/p&gt;
&lt;div class="math"&gt;
\begin{equation*}
\arg\min_{a_k} \sum_{-\infty}^\infty [x[n] - \sum_{k=1}^p a_k x[n-k]]^2
\end{equation*}
&lt;/div&gt;
&lt;p&gt;Because of its error criteria, LPC also has problems to represent the
phase of acoustic signals (by squaring the error, we are modeling the
spectral magnitude of the signal, and not the phase). For this reason,
LPC speech may sound artificial when resynthesized. More robust
methods are used nowadays, such as the code-excited linear prediction
(CELP) &lt;a class="citation-reference" href="#valin2006speex" id="id3"&gt;[valin2006speex]&lt;/a&gt;. These methods, for example, use
psychoacoustics-inspired techniques to shape the coding noise to
frequency regions where the human auditory system is more tolerant. In
CELP, the residual is not transmitted directly, but represented as
entries in two codebooks.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="wavelets"&gt;
&lt;h2&gt;Wavelets&lt;/h2&gt;
&lt;p&gt;Them main purpose of a wavelet transform is to decompose arbitrary
signals into localized contributions that can be labelled by a scale
(or resolution) parameter &lt;a class="citation-reference" href="#mallat1989theory" id="id4"&gt;[mallat1989theory]&lt;/a&gt;. The representation
achieved through the wavelet transform can be seen as hierarchical: at
a coarse resolution, we have an idea of “context”, while with highest
resolution we can see more details. This is achieved by decomposing
the original signal using a set of functions well-localized both in
time and frequency (the so-called wavelets).&lt;/p&gt;
&lt;p&gt;Discrete wavelet transforms are implemented as a cascade of digital
filters with transfer functions derived from a discrete &amp;quot;mother
wavelet&amp;quot;. The figure below shows an example. Check also the &lt;a class="reference internal" href="#notebook"&gt;notebook&lt;/a&gt;
for an example of wavelet decomposition of the audio signal shown
above.&lt;/p&gt;
&lt;div class="figure align-center"&gt;
&lt;img alt="" src="images/Wavelets_-_Filter_Bank.png" /&gt;
&lt;p class="caption"&gt;&lt;em&gt;Filter bank used by a discrete wavelet transform with 3 levels of decomposition (image from the&lt;/em&gt; &lt;a class="reference external" href="http://en.wikipedia.org/wiki/File:Wavelets_-_Filter_Bank.png"&gt;WikiMedia Commons&lt;/a&gt; &lt;em&gt;)&lt;/em&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="section" id="sparse-coding-and-dictionary-based-methods"&gt;
&lt;h2&gt;Sparse coding and dictionary-based methods&lt;/h2&gt;
&lt;p&gt;Sparse signal approximations are the basis for a variety of signal
processing techniques. Such approximations are usually employed with
the objective of having a signal representation that is more
meaningful, malleable, and robust to noise than the ones obtained by
standard transform methods &lt;a class="citation-reference" href="#sturm" id="id5"&gt;[Sturm]&lt;/a&gt;. The so-called
dictionary-based methods (DBM) decompose a signal into a linear
combination of waveforms through an approximation technique such as
Matching Pursuit (MP) &lt;a class="citation-reference" href="#mallat1993" id="id6"&gt;[Mallat1993]&lt;/a&gt; or Orthogonal Matching Pursuit
(OMP) &lt;a class="citation-reference" href="#pati1993" id="id7"&gt;[Pati1993]&lt;/a&gt;. The collection of waveforms that can be
selected for the linear combination is called a dictionary. This
dictionary is usually overcomplete, either because it is formed by
merging complete dictionaries or because the functions are chosen
arbitrarily.&lt;/p&gt;
&lt;p&gt;I will talk more about sparse coding and dictionary-based methods
later, since sparse coding is one of the methods we'll see in the
course.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="ipython-notebook"&gt;
&lt;span id="notebook"&gt;&lt;/span&gt;&lt;h2&gt;IPython notebook&lt;/h2&gt;
&lt;p&gt;An IPython notebook with examples for all the representations
described here (except sparse coding) is available on my &lt;a class="reference external" href="https://github.com/jfsantos/ift6266h14"&gt;GitHub
repo&lt;/a&gt;. You will need to install the packages &lt;a class="reference external" href="http://www.pybytes.com/pywavelets/"&gt;PyWavelets&lt;/a&gt; and
&lt;a class="reference external" href="https://github.com/cournape/talkbox"&gt;scikits.talkbox&lt;/a&gt; (both are available at PyPI) to be able to run it. If you just want to take a look without interacting with the code, you can access it &lt;a class="reference external" href="http://nbviewer.ipython.org/github/jfsantos/ift6266h14/blob/master/notebooks/Speech%20representation%20examples.ipynb"&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="references"&gt;
&lt;h2&gt;References&lt;/h2&gt;
&lt;table class="docutils citation" frame="void" id="stevens2005" rules="none"&gt;
&lt;colgroup&gt;&lt;col class="label" /&gt;&lt;col /&gt;&lt;/colgroup&gt;
&lt;tbody valign="top"&gt;
&lt;tr&gt;&lt;td class="label"&gt;&lt;a class="fn-backref" href="#id1"&gt;[Stevens2005]&lt;/a&gt;&lt;/td&gt;&lt;td&gt;&lt;ol class="first last upperalpha simple" start="19"&gt;
&lt;li&gt;&lt;ol class="first upperalpha" start="19"&gt;
&lt;li&gt;Stevens, J. Volkmann, and E. B. Newman, “A Scale for the Measurement of the Psychological Magnitude Pitch,” The Journal of the Acoustical Society of America, vol. 8, no. 3, pp. 185–190, Jun. 2005.&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/td&gt;&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;table class="docutils citation" frame="void" id="o1988linear" rules="none"&gt;
&lt;colgroup&gt;&lt;col class="label" /&gt;&lt;col /&gt;&lt;/colgroup&gt;
&lt;tbody valign="top"&gt;
&lt;tr&gt;&lt;td class="label"&gt;&lt;a class="fn-backref" href="#id2"&gt;[o1988linear]&lt;/a&gt;&lt;/td&gt;&lt;td&gt;&lt;ol class="first last upperalpha simple" start="4"&gt;
&lt;li&gt;O’Shaughnessy, “Linear predictive coding,” IEEE Potentials, vol. 7, no. 1, pp. 29–32, Feb. 1988.&lt;/li&gt;
&lt;/ol&gt;
&lt;/td&gt;&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;table class="docutils citation" frame="void" id="valin2006speex" rules="none"&gt;
&lt;colgroup&gt;&lt;col class="label" /&gt;&lt;col /&gt;&lt;/colgroup&gt;
&lt;tbody valign="top"&gt;
&lt;tr&gt;&lt;td class="label"&gt;&lt;a class="fn-backref" href="#id3"&gt;[valin2006speex]&lt;/a&gt;&lt;/td&gt;&lt;td&gt;J.-M. Valin, “Speex: a free codec for free speech,” in Australian National Linux Conference, Dunedin, New Zealand, 2006.&lt;/td&gt;&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;table class="docutils citation" frame="void" id="mallat1989theory" rules="none"&gt;
&lt;colgroup&gt;&lt;col class="label" /&gt;&lt;col /&gt;&lt;/colgroup&gt;
&lt;tbody valign="top"&gt;
&lt;tr&gt;&lt;td class="label"&gt;&lt;a class="fn-backref" href="#id4"&gt;[mallat1989theory]&lt;/a&gt;&lt;/td&gt;&lt;td&gt;&lt;ol class="first last upperalpha simple" start="19"&gt;
&lt;li&gt;&lt;ol class="first upperalpha" start="7"&gt;
&lt;li&gt;Mallat, “A theory for multiresolution signal decomposition: the wavelet representation,” Pattern Analysis and Machine Intelligence, IEEE Transactions on, vol. 11, no. 7, pp. 674–693, 1989.&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/td&gt;&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;table class="docutils citation" frame="void" id="sturm" rules="none"&gt;
&lt;colgroup&gt;&lt;col class="label" /&gt;&lt;col /&gt;&lt;/colgroup&gt;
&lt;tbody valign="top"&gt;
&lt;tr&gt;&lt;td class="label"&gt;&lt;a class="fn-backref" href="#id5"&gt;[Sturm]&lt;/a&gt;&lt;/td&gt;&lt;td&gt;&lt;ol class="first last upperalpha simple" start="2"&gt;
&lt;li&gt;&lt;ol class="first upperalpha" start="12"&gt;
&lt;li&gt;Sturm, C. Roads, A. McLeran, and J. J. Shynk, “Analysis, Visualization, and Transformation of Audio Signals Using Dictionary-based Methods†,” Journal of New Music Research, vol. 38, no. 4, pp. 325–341, 2009.&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/td&gt;&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;table class="docutils citation" frame="void" id="mallat1993" rules="none"&gt;
&lt;colgroup&gt;&lt;col class="label" /&gt;&lt;col /&gt;&lt;/colgroup&gt;
&lt;tbody valign="top"&gt;
&lt;tr&gt;&lt;td class="label"&gt;&lt;a class="fn-backref" href="#id6"&gt;[Mallat1993]&lt;/a&gt;&lt;/td&gt;&lt;td&gt;&lt;ol class="first last upperalpha simple" start="19"&gt;
&lt;li&gt;&lt;ol class="first upperalpha" start="7"&gt;
&lt;li&gt;Mallat and Z. Zhang, “Matching pursuits with time-frequency dictionaries,” IEEE Transactions on Signal Processing, vol. 41, no. 12, pp. 3397–3415, Dec. 1993.&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/td&gt;&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;table class="docutils citation" frame="void" id="pati1993" rules="none"&gt;
&lt;colgroup&gt;&lt;col class="label" /&gt;&lt;col /&gt;&lt;/colgroup&gt;
&lt;tbody valign="top"&gt;
&lt;tr&gt;&lt;td class="label"&gt;&lt;a class="fn-backref" href="#id7"&gt;[Pati1993]&lt;/a&gt;&lt;/td&gt;&lt;td&gt;&lt;ol class="first last upperalpha simple" start="25"&gt;
&lt;li&gt;&lt;ol class="first upperalpha" start="3"&gt;
&lt;li&gt;Pati, R. Rezaiifar, and P. S. Krishnaprasad, “Orthogonal matching pursuit: Recursive function approximation with applications to wavelet decomposition,” in Signals, Systems and Computers, 1993. 1993 Conference Record of The Twenty-Seventh Asilomar Conference on, 1993, pp. 40–44.&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/td&gt;&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">jfsantos</dc:creator><pubDate>Sat, 01 Feb 2014 18:00:00 -0500</pubDate><guid>tag:,2014-02-01:initial_representation.html</guid><category>ift6266</category></item><item><title>Personal research journal: deep learning for speech synthesis</title><link>/intro_ift6266.html</link><description>&lt;p&gt;This is the introduction to a series of reports on my experiments on
deep learning methods for speech synthesis. These experiments are part
of my coursework for Dr. Yoshua Bengio's &lt;a class="reference external" href="http://ift6266h14.wordpress.com"&gt;Representation Learning&lt;/a&gt;
course at Université de Montréal. All the related code is going to be
posted at a &lt;a class="reference external" href="https://github.com/jfsantos/ift6266h14"&gt;GitHub repository&lt;/a&gt; as well.&lt;/p&gt;
&lt;p&gt;Please visit the &lt;a class="reference external" href="/tag/ift6266.html"&gt;ift6266 tag page&lt;/a&gt; for a list of all the posts
related to this project.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">jfsantos</dc:creator><pubDate>Sat, 25 Jan 2014 15:00:00 -0500</pubDate><guid>tag:,2014-01-25:intro_ift6266.html</guid><category>ift6266</category></item><item><title>My Emacs configuration for MATLAB, LaTeX, and Python</title><link>/emacs-config.html</link><description>&lt;p&gt;Emacs is a main part of my daily life. I write code and papers using
it, so you could say that anything that my research will leave
(hopefully) for the next generations was probably typed using it. Not
anybody loves Emacs, and I will not try to convince you to use
it. This post is just a brief overview of all the Emacs goodness I
found during the last years and I believe you should be using too, if
you use Emacs to write anything in MATLAB, LaTeX and Python.&lt;/p&gt;
&lt;p&gt;First things first: &lt;a class="reference external" href="http://batsov.com/prelude/"&gt;Prelude&lt;/a&gt; is the base configuration I have been
using during the last years. Prelude is a sane base configuration that
includes a lot of good things and saves a &lt;strong&gt;lot&lt;/strong&gt; of time. If you use
any UNIX-like operating system, installing it is a matter of running
this command:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;curl -L http://git.io/epre | sh
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;as long as you already have Emacs 24 (if you are still using Emacs 23,
go on and update it ASAP!). Note that this installs it to the default
Emacs configuration folder (&lt;tt class="docutils literal"&gt;&lt;span class="pre"&gt;$HOME/.emacs.d&lt;/span&gt;&lt;/tt&gt;).&lt;/p&gt;
&lt;p&gt;I know that I said Prelude is a sane base configuration, but I don't
like two things it includes by default: &lt;tt class="docutils literal"&gt;&lt;span class="pre"&gt;guru-mode&lt;/span&gt;&lt;/tt&gt; (which basically
disables keyboard arrows to force you to learn how to navigate using
default Emacs commands) and &lt;tt class="docutils literal"&gt;&lt;span class="pre"&gt;whitespace-mode&lt;/span&gt;&lt;/tt&gt; (which renders
whitespace characters with visible glyphs). To disable these (and to
add anything to a Prelude-based Emacs configuration), edit (you may
have to create it first) &lt;tt class="docutils literal"&gt;&lt;span class="pre"&gt;$HOME/.emacs.d/personal/init.el&lt;/span&gt;&lt;/tt&gt; and add
the following lines:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;;; Disable guru-mode (I like using arrows :p)
(setq prelude-guru nil)
;; Disable whitespace-mode
(setq prelude-whitespace nil)
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Another thing you should do is to enable the Prelude modules you want
to use (and disable the ones you are never going to need). To do it,
copy &lt;tt class="docutils literal"&gt;&lt;span class="pre"&gt;$HOME/.emacs.d/sample/prelude-modules.el&lt;/span&gt;&lt;/tt&gt; to
&lt;tt class="docutils literal"&gt;&lt;span class="pre"&gt;$HOME/.emacs.d/prelude-modules.el&lt;/span&gt;&lt;/tt&gt;, remove the comment symbols
(&lt;tt class="docutils literal"&gt;;&lt;/tt&gt;) from the modules you want to use and comment the ones you
don't want.&lt;/p&gt;
&lt;div class="section" id="matlab"&gt;
&lt;h2&gt;MATLAB&lt;/h2&gt;
&lt;p&gt;While MATLAB has a decent GUI (and a nice graphical debugger), the
editor is not very good (it does code completion, some hinting, and
that's all). Also, in my experience, the GUI is not reliable under
Linux. &lt;a class="reference external" href="http://matlab-emacs.sourceforge.net/"&gt;matlab-emacs&lt;/a&gt; is an Emacs package that adds a major mode to
edit MATLAB code and also enables a MATLAB shell from inside Emacs. To
install it, you can run this &lt;a class="reference external" href="http://matlab-emacs.cvs.sourceforge.net/viewvc/*checkout*/matlab-emacs/matlab-emacs/dl_emacs_support.m?revision=1.2&amp;amp;pathrev=MAIN"&gt;script&lt;/a&gt; from MATLAB to download all the
files and save them to &lt;tt class="docutils literal"&gt;&lt;span class="pre"&gt;$HOME/.emacs.d/personal/matlab-emacs&lt;/span&gt;&lt;/tt&gt;. Then,
to make Emacs automatically open &lt;tt class="docutils literal"&gt;.m&lt;/tt&gt; files using &lt;tt class="docutils literal"&gt;&lt;span class="pre"&gt;matlab-mode&lt;/span&gt;&lt;/tt&gt;
and configure &lt;tt class="docutils literal"&gt;&lt;span class="pre"&gt;matlab-shell&lt;/span&gt;&lt;/tt&gt;, add the following lines to
&lt;tt class="docutils literal"&gt;&lt;span class="pre"&gt;$HOME/.emacs.d/personal/init.el&lt;/span&gt;&lt;/tt&gt;:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;;; Setting up matlab-mode
(add-to-list &amp;#39;load-path &amp;quot;~/.emacs.d/personal/matlab-emacs&amp;quot;)
(load-library &amp;quot;matlab-load&amp;quot;)
(custom-set-variables
 &amp;#39;(matlab-shell-command-switches &amp;#39;(&amp;quot;-nodesktop -nosplash&amp;quot;)))
(add-hook &amp;#39;matlab-mode-hook &amp;#39;auto-complete-mode)
(setq auto-mode-alist
    (cons
     &amp;#39;(&amp;quot;\\.m$&amp;quot; . matlab-mode)
     auto-mode-alist))
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;There is much more in &lt;tt class="docutils literal"&gt;&lt;span class="pre"&gt;matlab-emacs&lt;/span&gt;&lt;/tt&gt; than just code completion and
the shell. You can use Emacs to debug MATLAB code, send selected
regions to execute on MATLAB, and much more. Take a look at the following articles to have a better idea of the other features:&lt;/p&gt;
&lt;p&gt;&lt;a class="reference external" href="http://blog.angjookanazawa.com/post/8815280589/productivity-matlab-emacs-integration-more"&gt;http://blog.angjookanazawa.com/post/8815280589/productivity-matlab-emacs-integration-more&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a class="reference external" href="http://blogs.mathworks.com/community/2009/09/14/matlab-emacs-integration-is-back/"&gt;http://blogs.mathworks.com/community/2009/09/14/matlab-emacs-integration-is-back/&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="python"&gt;
&lt;h2&gt;Python&lt;/h2&gt;
&lt;p&gt;Prelude includes a Python module, but if you look deeper you will see
it just enables &lt;tt class="docutils literal"&gt;&lt;span class="pre"&gt;subword-mode&lt;/span&gt;&lt;/tt&gt; (to make word moving commands jump
inside CamelCase words) and disables &lt;tt class="docutils literal"&gt;&lt;span class="pre"&gt;electric-indent-mode&lt;/span&gt;&lt;/tt&gt; (to
disable automatic indentation after a line break), which is not much
(and you may even disagree about this setup!). If you would like code
completion, refactoring, code hinting, code navigation, inline
documentation, and more, &lt;a class="reference external" href="https://github.com/jorgenschaefer/elpy"&gt;Elpy&lt;/a&gt; is a good option. Installing it
depends on two steps, as it has both a Emacs and a Python package. You
can simply follow these &lt;a class="reference external" href="https://github.com/jorgenschaefer/elpy/wiki/Installation"&gt;instructions&lt;/a&gt; and be happy with a boatload
of features! You will also need to install either &lt;a class="reference external" href="http://rope.sourceforge.net/"&gt;Rope&lt;/a&gt; or &lt;a class="reference external" href="https://github.com/davidhalter/jedi"&gt;Jedi&lt;/a&gt;
(from the Python side), which Elpy uses for code completion. Rope
appears to have better refactoring features, so that's what I
use. Note that you have to install Elpy and Rope to a place on your
default &lt;tt class="docutils literal"&gt;PYTHONPATH&lt;/tt&gt;, which you can do by running &lt;tt class="docutils literal"&gt;pip
install &lt;span class="pre"&gt;--user&lt;/span&gt; elpy rope&lt;/tt&gt;. This will install the packages to
&lt;tt class="docutils literal"&gt;&lt;span class="pre"&gt;$HOME/.local/lib/python2.7/site-packages&lt;/span&gt;&lt;/tt&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="latex"&gt;
&lt;h2&gt;LaTeX&lt;/h2&gt;
&lt;p&gt;&lt;a class="reference external" href="https://www.gnu.org/software/auctex/"&gt;AUCTeX&lt;/a&gt;, an amazing package for writing LaTeX using Emacs, is
already included on Prelude (as long as you activate the LaTeX
module). It supports auto-completing LaTex expressions and has a
&amp;quot;magic compilation&amp;quot; command: &lt;tt class="docutils literal"&gt;&lt;span class="pre"&gt;C-c&lt;/span&gt; &lt;span class="pre"&gt;C-c&lt;/span&gt;&lt;/tt&gt;. It checks the status of your
folder and runs either latex/pdflatex, bibtex, or opens the output
file.&lt;/p&gt;
&lt;p&gt;RefTeX is another extension but it ships with Emacs since Emacs
24.3. If your &lt;tt class="docutils literal"&gt;.tex&lt;/tt&gt; file includes a reference to a &lt;tt class="docutils literal"&gt;.bib&lt;/tt&gt; file,
you can use the command &lt;tt class="docutils literal"&gt;&lt;span class="pre"&gt;C-c&lt;/span&gt; [&lt;/tt&gt; to add a citation to any of the
references listed on that file.&lt;/p&gt;
&lt;p&gt;I also recommend using &lt;tt class="docutils literal"&gt;&lt;span class="pre"&gt;writegood-mode&lt;/span&gt;&lt;/tt&gt; to detect use of weasel
words, duplicate words and passive voice. You can install it by simply
running the command &lt;tt class="docutils literal"&gt;&lt;span class="pre"&gt;M-x&lt;/span&gt; &lt;span class="pre"&gt;package-install&lt;/span&gt; &lt;span class="pre"&gt;writegood-mode&lt;/span&gt;&lt;/tt&gt; directly
from Emacs (as long as you are using Prelude as described previously).&lt;/p&gt;
&lt;p&gt;This is the configuration I use for fine-tuning AUCTeX (use pdflatex by default, use RefTeX, configure viewers, enable &lt;tt class="docutils literal"&gt;&lt;span class="pre"&gt;writegood-mode&lt;/span&gt;&lt;/tt&gt;):&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;;; LaTeX configuration
(setq TeX-auto-save t)
(setq TeX-parse-self t)
(setq-default TeX-master nil)

(add-hook &amp;#39;LaTeX-mode-hook &amp;#39;visual-line-mode)
(add-hook &amp;#39;LaTeX-mode-hook &amp;#39;flyspell-mode)
(add-hook &amp;#39;LaTeX-mode-hook &amp;#39;LaTeX-math-mode)
(add-hook &amp;#39;LaTeX-mode-hook &amp;#39;TeX-source-correlate-mode)

(add-hook &amp;#39;LaTeX-mode-hook &amp;#39;turn-on-reftex)
(setq reftex-plug-into-AUCTeX t)
(setq TeX-PDF-mode t)

(setq TeX-output-view-style
    (quote
     ((&amp;quot;^pdf$&amp;quot; &amp;quot;.&amp;quot; &amp;quot;evince -f %o&amp;quot;)
      (&amp;quot;^html?$&amp;quot; &amp;quot;.&amp;quot; &amp;quot;iceweasel %o&amp;quot;))))

;; Setting up writegood-mode
(require &amp;#39;writegood-mode)
(global-set-key &amp;quot;\C-cg&amp;quot; &amp;#39;writegood-mode)
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;div class="section" id="extras"&gt;
&lt;h2&gt;Extras&lt;/h2&gt;
&lt;p&gt;If you want to tinker and add your own features to Emacs, you will
probably want to learn some Emacs Lisp. This is a nice and brief
tutorial to get a grasp of it:
&lt;a class="reference external" href="http://bzg.fr/learn-emacs-lisp-in-15-minutes.html"&gt;http://bzg.fr/learn-emacs-lisp-in-15-minutes.html&lt;/a&gt; (and this should be
useful even if you don't want to create your own extensions, but just
add some fancy things to your Emacs config!).&lt;/p&gt;
&lt;/div&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">jfsantos</dc:creator><pubDate>Sun, 28 Jul 2013 12:00:00 -0400</pubDate><guid>tag:,2013-07-28:emacs-config.html</guid><category>development</category></item><item><title>WinPython: a portable scientific Python distribution for Windows</title><link>/winpython.html</link><description>&lt;p&gt;Recently I developed a &lt;a class="reference external" href="http://bitbucket.org/jfsantos/sitest"&gt;piece of software&lt;/a&gt; for administering speech
intelligibility listening tests and discovered a small issue: I am not
an administrator of the computer that I am going to use for the tests,
so I cannot install any software. Since the computer is running
Windows, there is no standard Python distribution there. I already
knew &lt;a class="reference external" href="http://portablepython.com"&gt;Portable Python&lt;/a&gt;, which is (surprise!) a portable Python
distribution that can be run from a USB storage device. However, the
first version of my code was using PyAudio, which was not included in
this distribution. Since this module uses binary wrappers to
PortAudio, installation is not as straightforward as copying a bunch
of .py files, and I could not find out how to install it to my USB
drive... but then, WinPython came to the rescue!&lt;/p&gt;
&lt;p&gt;&lt;a class="reference external" href="https://code.google.com/p/winpython/"&gt;WinPython&lt;/a&gt; is a complete
NumPy/SciPy portable development environment, which also includes an
IDE (&lt;a class="reference external" href="https://code.google.com/p/spyderlib/"&gt;Spyder&lt;/a&gt;), my favorite
version control system (Mercurial) and Qt tools for GUI
development. However, what made me like it so much is the WinPython
Package Manager (&lt;a class="reference external" href="https://code.google.com/p/winpython/wiki/WPPM"&gt;WPPM&lt;/a&gt;), which allows one
to install any packages that can be installed with &lt;em&gt;easy_install&lt;/em&gt;,
&lt;em&gt;pip&lt;/em&gt;, or &lt;em&gt;distutils&lt;/em&gt; standard installers. The latter is specially
interesting because &lt;em&gt;distutils&lt;/em&gt; installers include binary
dependencies, so you do not need a C/C++ compiler. If scientific
computing is your thing, maybe the library you need is just a visit to
&lt;a class="reference external" href="http://www.lfd.uci.edu/~gohlke/pythonlibs/"&gt;Christoph Gohlke's page&lt;/a&gt; away (which was my case
with PyAudio).&lt;/p&gt;
&lt;p&gt;Another interesting feature is the possibility of converting your
portable distribution to a more &amp;quot;conventional&amp;quot; Python installation on
Windows by adding some keys to the registry (this is also done through
the WinPython Control Panel). Also, both Python 2.7 and Python 3.3 are
available, with support to 32 and 64 bit Windows XP/7/8.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">jfsantos</dc:creator><pubDate>Tue, 25 Jun 2013 21:00:00 -0400</pubDate><guid>tag:,2013-06-25:winpython.html</guid><category>development</category></item><item><title>Using Vagrant to make reproducible development and test environments</title><link>/using-vagrant.html</link><description>&lt;p&gt;Working with research-grade software is hard sometimes. We all have
heard before (more than once, probably) the &amp;quot;it worked perfectly on my
computer&amp;quot; story. The problem is that setting up a development
environment takes a lot of time, and usually you'll end up forgetting
which libraries and pieces of software you needed to install to get
things running... and if you cannot figure out what did you have to do
to get it working, let alone your colleagues that may need to use your
code someday!&lt;/p&gt;
&lt;p&gt;One simple way to try to mitigate this problem is to use virtual
machines (VMs) as development and test environments. By developing
your code on a VM, you can later distribute it instead of distributing
your code only. I know that's not how serious people develop software,
but let's be honest here: many research-grade software is developed on
a &amp;quot;code and forget&amp;quot; paradigm. As soon as you publish your
paper/thesis/whatever, you'll forget about it. Then, you will update
your operating system, hardware, libraries. Then, months or years
later, you will come up with a wonderful idea and think &amp;quot;I already
have software for doing half of that!&amp;quot; just to find out that it does
not work anymore... That's not nice.&lt;/p&gt;
&lt;p&gt;Of course, developing and distributing software using VMs has other
advantages. For collaborative projects, you can make sure everybody is
working with the same versions of the libraries and
compilers/interpreters. Also, it is a way of having access to another
operating system such as Linux (if, like me, you are stuck with
Windows for the time being).&lt;/p&gt;
&lt;p&gt;One way of following this approach would be to have a &amp;quot;base&amp;quot; virtual
machine and then, every time you start a new software project, you do
this:&lt;/p&gt;
&lt;ol class="arabic simple"&gt;
&lt;li&gt;Make a copy from the base environment.&lt;/li&gt;
&lt;li&gt;Install everything that you need on that copy, manually.&lt;/li&gt;
&lt;li&gt;Develop your software, tests, etc.&lt;/li&gt;
&lt;li&gt;Package the result and distribute (or archive) it.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;However, making copies and installing everything by hand is boring, so
I chose to use an amazing tool called Vagrant to do a lot of this
boring work for me. It is able to download base virtual machines (for
example, a barebones Ubuntu VM), configure them, and install software
on it automatically. All you need to do is write a couple of
configuration files.&lt;/p&gt;
&lt;div class="section" id="setting-up-vagrant-on-windows"&gt;
&lt;h2&gt;Setting up Vagrant on Windows&lt;/h2&gt;
&lt;p&gt;In order to set up and properly use Vagrant on Windows, you will need
to install it and also VirtualBox, which is the default VM
provider. It is also recommended that you install Git, which comes
with a SSH client for Windows (that you can use to access the VM
instead of using the default VirtualBox interface). You can find the
installers on the following URLs:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;Vagrant &lt;a class="reference external" href="http://downloads.vagrantup.com/tags/v1.2.2"&gt;http://downloads.vagrantup.com/tags/v1.2.2&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;VirtualBox &lt;a class="reference external" href="http://www.virtualbox.org/wiki/Downloads"&gt;http://www.virtualbox.org/wiki/Downloads&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Git &lt;a class="reference external" href="http://www.git-scm.com/downloads"&gt;http://www.git-scm.com/downloads&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;By default, Vagrant will keep the downloaded base VMs at your home
folder (&lt;tt class="docutils literal"&gt;&lt;span class="pre"&gt;%USERPROFILE%\.vagrant.d&lt;/span&gt;&lt;/tt&gt;), but you can choose another
folder by setting the environment variable &lt;tt class="docutils literal"&gt;%VAGRANT_HOME%&lt;/tt&gt; on
Windows. Also, VirtualBox will store your VMs at
&lt;tt class="docutils literal"&gt;&lt;span class="pre"&gt;%USERPROFILE%\VirtualBox&lt;/span&gt; VMs&lt;/tt&gt;, but you can change it by opening
VirtualBox preferences and changing the default VM folder. As both
folders will end up storing a lot of data depending on how many VMs
you use, it may be interesting to change both before starting to use
the approach presented here.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="example-vm"&gt;
&lt;h2&gt;Example VM&lt;/h2&gt;
&lt;p&gt;I wrote a simple configuration file to use as an example for this
post. The machine configuration is all done on this file (which is
called &lt;em&gt;Vagrantfile&lt;/em&gt;) except for the software installation, which is
done using a simple shell script. As you can see below, the
configuration is very simple. To test it, just download both files,
fire up a Git Bash session, go to this directory and run &lt;tt class="docutils literal"&gt;vagrant
up&lt;/tt&gt;. This will start up the virtual machine and install all the
required software on it.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;Vagrant::Config.run do |config|
  # All Vagrant configuration is done here. The most common configuration
  # options are documented and commented below. For a complete reference,
  # please see the online documentation at vagrantup.com.

  # Every Vagrant virtual environment requires a box to build off of.
  config.vm.box = &amp;quot;precise32&amp;quot;

  # The url from where the &amp;#39;config.vm.box&amp;#39; box will be fetched if it
  # doesn&amp;#39;t already exist on the user&amp;#39;s system.
  config.vm.box_url = &amp;quot;http://files.vagrantup.com/precise32.box&amp;quot;

  # Amount of RAM on the VM
  config.vm.customize [&amp;quot;modifyvm&amp;quot;, :id, &amp;quot;--memory&amp;quot;, 512] # 512 MB RAM

  # Boot with a GUI so you can see the screen. (Default is headless)
  # config.vm.boot_mode = :gui

  # Assign this VM to a host-only network IP, allowing you to access it
  # via the IP. Host-only networks can talk to the host machine as well as
  # any other machines on the same network, but cannot be accessed (through this
  # network interface) by any external networks.
  # config.vm.network :hostonly, &amp;quot;192.168.33.10&amp;quot;

  # Assign this VM to a bridged network, allowing you to connect directly to a
  # network using the host&amp;#39;s network device. This makes the VM appear as another
  # physical device on your network.
  config.vm.network :bridged

  # Forward a port from the guest to the host, which allows for outside
  # computers to access the VM, whereas host only networking does not.
  config.vm.forward_port 8000, 8000

  # Share an additional folder to the guest VM. The first argument is
  # an identifier, the second is the path on the guest to mount the
  # folder, and the third is the path on the host to the actual folder.
  # config.vm.share_folder &amp;quot;v-data&amp;quot;, &amp;quot;/vagrant_data&amp;quot;, &amp;quot;../data&amp;quot;

  # Enable provisioning with a shell script. Add the sequence of
  # commands you want to run to provision the VM to provision.sh
  config.vm.provision :shell, :path =&amp;gt; &amp;quot;provision.sh&amp;quot;
end
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;For this example, we will simply add commands to install GNU Octave on
the VM to the file &lt;tt class="docutils literal"&gt;provision.sh&lt;/tt&gt;:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="nb"&gt;export &lt;/span&gt;&lt;span class="nv"&gt;DEBIAN_FRONTEND&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;noninteractive
apt-get update &amp;gt; /dev/null
apt-get -y install octave
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;div class="section" id="accessing-the-vm-transferring-files-between-it-and-the-host"&gt;
&lt;h2&gt;Accessing the VM, transferring files between it and the host&lt;/h2&gt;
&lt;p&gt;The command &lt;tt class="docutils literal"&gt;vagrant up&lt;/tt&gt; leaves the machine running after the
setup. To access it, just run &lt;tt class="docutils literal"&gt;ssh &lt;span class="pre"&gt;-p&lt;/span&gt; 2222 vagrant&amp;#64;localhost&lt;/tt&gt; (the
password is &lt;em&gt;vagrant&lt;/em&gt;). You can also access the VM using VirtualBox's
interface. In case you need graphical output or audio, this is the
simplest way of accessing it (you should add &lt;tt class="docutils literal"&gt;config.vm.boot_mode =
gui&lt;/tt&gt; to your Vagrantfile, too). Otherwise, SSH will work fine.&lt;/p&gt;
&lt;p&gt;Another magic trick done by Vagrant is to set up shared folders, so
sending files to the VM (and getting them from the VM) is easy. The
folder where you store the configuration files is mounted with
read-write access on the VM at &lt;tt class="docutils literal"&gt;/vagrant&lt;/tt&gt;.&lt;/p&gt;
&lt;p&gt;To shut down the VM, use the commands &lt;tt class="docutils literal"&gt;vagrant halt&lt;/tt&gt; or &lt;tt class="docutils literal"&gt;vagrant
suspend&lt;/tt&gt; (the latter will save the machine state, which will be
reloaded on the next time you start it).&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="final-considerations"&gt;
&lt;h2&gt;Final considerations&lt;/h2&gt;
&lt;p&gt;That's it, basically. You can use &lt;a class="reference external" href="https://bitbucket.org/jfsantos/vagrant-example"&gt;these files&lt;/a&gt; as a starting point to
design your own virtual machines. There are more complex ways of
provisioning VMs, using tools such as Chef and Puppet, but that's out
of the scope of this blog post. For an excellent example using Puppet,
check out this &lt;a class="reference external" href="https://github.com/gavinln/stats_py_vm"&gt;Github project&lt;/a&gt; (which, in fact, is the project that
inspired me to use VMs for software development in my research!). It
sets up a VM with statistics and numerical libraries for Python.&lt;/p&gt;
&lt;/div&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">jfsantos</dc:creator><pubDate>Sun, 19 May 2013 16:00:00 -0400</pubDate><guid>tag:,2013-05-19:using-vagrant.html</guid><category>development</category></item></channel></rss>